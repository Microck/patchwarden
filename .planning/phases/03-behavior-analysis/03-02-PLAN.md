---
phase: 03-behavior-analysis
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/mod_sentinel/llm/client.py
  - src/mod_sentinel/llm/json_extract.py
  - src/mod_sentinel/llm/behavior_contract.py
  - src/mod_sentinel/agents/behavior_agent.py
  - tests/test_llm_json_extract.py
  - tests/test_behavior_contract.py
  - tests/test_behavior_agent_parsing.py
autonomous: true

must_haves:
  truths:
    - "LLM responses are parsed robustly into the expected JSON contract"
    - "Missing/invalid fields degrade safely to 'unknown' rather than crashing"
  artifacts:
    - path: "src/mod_sentinel/llm/behavior_contract.py"
      provides: "Validation contract for behavior prediction"
  key_links:
    - from: "src/mod_sentinel/agents/behavior_agent.py"
      to: "src/mod_sentinel/llm/json_extract.py"
      via: "JSON extraction"
      pattern: "json_extract"
    - from: "src/mod_sentinel/agents/behavior_agent.py"
      to: "src/mod_sentinel/llm/behavior_contract.py"
      via: "Normalization/validation"
      pattern: "behavior_contract"
---

<objective>
Harden behavior prediction parsing/validation so real LLM responses (markdown, extra text) don't break the pipeline.

Purpose: Make the LLM integration reliable enough for a demo.
Output: JSON extraction + validation contract + tests.
</objective>

<execution_context>
@/home/ubuntu/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/REQUIREMENTS.md
@.planning/phases/03-behavior-analysis/03-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement robust JSON extraction helper for LLM outputs</name>
  <files>
src/mod_sentinel/llm/json_extract.py
tests/test_llm_json_extract.py
  </files>
  <action>
Implement a utility that extracts the first JSON object from arbitrary text (e.g., fenced code blocks, leading commentary).

Rules:
- Prefer fenced ```json blocks if present
- Else scan for the first balanced `{ ... }` region
- If extraction fails, raise a typed error with the raw text truncated for logs

Tests should cover:
- pure JSON
- markdown + fenced json
- extra text before/after
  </action>
  <verify>python3 -m pytest -q -k llm_json_extract --maxfail=1</verify>
  <done>Helper reliably extracts JSON from common LLM response formats.</done>
</task>

<task type="auto">
  <name>Task 2: Define behavior contract + safe degradation rules</name>
  <files>
src/mod_sentinel/llm/behavior_contract.py
tests/test_behavior_contract.py
  </files>
  <action>
Define a validation/normalization layer for behavior predictions:
- Accept partial fields and normalize to a complete internal representation.
- Any unparseable section becomes `unknown` with `confidence: 0` for that section.

This is explicitly to avoid demo-breaking failures when the real LLM deviates slightly.
  </action>
  <verify>python3 -m pytest -q -k behavior_contract --maxfail=1</verify>
  <done>Invalid/missing fields do not crash; normalization produces consistent output.</done>
</task>

<task type="auto">
  <name>Task 3: Wire json_extract + behavior_contract into BehaviorAgent parsing</name>
  <files>
src/mod_sentinel/llm/client.py
src/mod_sentinel/agents/behavior_agent.py
tests/test_behavior_agent_parsing.py
  </files>
  <action>
Update `BehaviorAgent` to treat the LLM output as untrusted text and explicitly run:
- `json_extract` to obtain a JSON object even when the response is wrapped in markdown (```json fences) or has leading/trailing commentary.
- `behavior_contract` normalization to produce a complete, safe internal representation before constructing `BehaviorPrediction`.

If `LLMClient` currently only returns a dict, extend it to expose a raw-text completion method (e.g., `complete_text(...) -> str`) and use that in `BehaviorAgent` so json_extract is exercised in the real code path.

Add an integration test that uses a stub client returning markdown-wrapped JSON and asserts:
- `BehaviorAgent.predict(...)` returns a `BehaviorPrediction` (no exception)
- missing/invalid fields degrade to `unknown` per the contract
  </action>
  <verify>python3 -m pytest -q -k behavior_agent_parsing --maxfail=1</verify>
  <done>Markdown-wrapped JSON responses do not break behavior prediction parsing (robust extraction + contract normalization are active in BehaviorAgent).</done>
</task>

</tasks>

<verification>
- `python3 -m pytest -q` passes
- Behavior parsing is resilient to markdown-wrapped JSON
</verification>

<success_criteria>
- Real provider can be swapped in later with low risk of runtime parsing failures
</success_criteria>

<output>
After completion, create `.planning/phases/03-behavior-analysis/03-02-SUMMARY.md`
</output>
